# Property Scraper Setup Guide

This guide explains how to set up and run the Python property scraper.

## üóÑÔ∏è Database Setup

### Step 1: Create Properties Table

1. Go to [Supabase SQL Editor](https://app.supabase.com/project/cqoqbbdypebzawgmtazv/sql)
2. Copy and paste the contents of `database-properties-setup.sql`
3. Click **Run** to execute

This creates the `properties` table with all necessary indexes and RLS policies.

### Step 2: Get Service Role Key

1. Go to [Supabase API Settings](https://app.supabase.com/project/cqoqbbdypebzawgmtazv/settings/api)
2. Copy the **service_role** key (not the anon key!)
3. Add it to your `.env.local`:

```bash
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key-here
```

‚ö†Ô∏è **Important**: The service role key bypasses RLS. Keep it secret! Never commit it to git.

## üêç Python Script Setup

### Step 1: Install Python Dependencies

```bash
cd scripts
pip install -r requirements.txt
```

Or install individually:
```bash
pip install requests beautifulsoup4 python-dotenv supabase
```

### Step 2: Run the Scraper

```bash
# From project root
python scripts/scraper.py
```

This will:
1. Scrape all properties from javeahomefinders.com
2. Save them to `scraped-properties.json` (backup)
3. Upload them to your Supabase database

## üìÖ Automation Options

### Option 1: Local Cron Job (Simple)

Run the scraper daily on your local machine:

**Mac/Linux:**
```bash
# Edit crontab
crontab -e

# Add this line to run daily at 3 AM
0 3 * * * cd /path/to/JaveaRealEstate && /usr/bin/python3 scripts/scraper.py >> scraper.log 2>&1
```

**Windows (Task Scheduler):**
1. Open Task Scheduler
2. Create Basic Task
3. Trigger: Daily at 3:00 AM
4. Action: Start a program
5. Program: `python`
6. Arguments: `scripts/scraper.py`
7. Start in: `C:\path\to\JaveaRealEstate`

### Option 2: Supabase Edge Function with Cron (Advanced)

Supabase can run Edge Functions on a schedule using pg_cron.

**Create Edge Function:**

1. Install Supabase CLI:
```bash
npm install -g supabase
```

2. Initialize Supabase:
```bash
supabase init
```

3. Create the function:
```bash
supabase functions new scrape-properties
```

4. Add scraper code to `supabase/functions/scrape-properties/index.ts`

5. Deploy:
```bash
supabase functions deploy scrape-properties
```

6. Set up pg_cron in Supabase SQL Editor:
```sql
-- Run daily at 3 AM
SELECT cron.schedule(
  'scrape-properties-daily',
  '0 3 * * *',
  $$
  SELECT net.http_post(
    url:='https://your-project-ref.supabase.co/functions/v1/scrape-properties',
    headers:='{"Authorization": "Bearer YOUR_ANON_KEY"}'::jsonb
  );
  $$
);
```

### Option 3: GitHub Actions (Free, Cloud-based)

Run the scraper on GitHub's servers:

1. Create `.github/workflows/scraper.yml`:
```yaml
name: Daily Property Scraper

on:
  schedule:
    - cron: '0 3 * * *'  # 3 AM daily
  workflow_dispatch:  # Manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r scripts/requirements.txt

      - name: Run scraper
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: python scripts/scraper.py
```

2. Add secrets in GitHub:
   - Go to repo Settings ‚Üí Secrets ‚Üí Actions
   - Add `NEXT_PUBLIC_SUPABASE_URL`
   - Add `SUPABASE_SERVICE_ROLE_KEY`

## üîÑ Updating Your Frontend

### Option 1: Fetch from Supabase (Recommended)

Update your frontend to fetch properties from Supabase instead of static data.

**Create `lib/supabase/queries.ts`:**
```typescript
import { createClient } from '@/lib/supabase/client';
import type { Property } from '@/data/properties';

export async function getProperties(): Promise<Property[]> {
  const supabase = createClient();

  const { data, error } = await supabase
    .from('properties')
    .select('*')
    .eq('status', 'available')
    .order('created_at', { ascending: false });

  if (error) throw error;
  return data as Property[];
}
```

### Option 2: Keep Static Data (Current)

If you want to keep using static files, you can export from Supabase:

```bash
# Add this to your Python script
def export_to_typescript(properties: List[Dict]) -> None:
    """Export properties to TypeScript file."""
    ts_content = f'''// Auto-generated by scraper.py
// Last updated: {datetime.now().isoformat()}

import type {{ Property }} from './properties';

export const allProperties: Property[] = {json.dumps(properties, indent=2)};
'''

    with open('../data/scraped-properties.ts', 'w') as f:
        f.write(ts_content)
```

## üìä Monitoring

View properties in Supabase:
1. Go to [Table Editor](https://app.supabase.com/project/cqoqbbdypebzawgmtazv/editor)
2. Select `properties` table
3. You'll see all scraped properties with their data

## üõ†Ô∏è Troubleshooting

**"Missing Supabase credentials" error:**
- Make sure `.env.local` has `SUPABASE_SERVICE_ROLE_KEY`
- Make sure the key is the service role key, not the anon key

**Properties not appearing in table:**
- Check RLS policies are correct
- Verify service role key has proper permissions
- Check scraper logs for upload errors

**Scraper failing:**
- Check website HTML structure hasn't changed
- Verify internet connection
- Check for rate limiting (script includes 1-second delays)
